import itertools
import os
import collections
from snakemake.utils import R
from snakemake.exceptions import MissingInputException
# Snakemake Base location
try:
    NGS_PIPELINE=os.environ['NGS_PIPELINE']
except KeyError:
    NGS_PIPELINE="/data/khanlab/projects/patidar/Serpentine"
    pass
#Basic utility functions
try:
    WORK_DIR=os.environ['WORK_DIR']
except KeyError:
	print("I can not locate your work directory")
	quit()

try:
    DATA_DIR=os.environ['DATA_DIR']
except KeyError:
        print("I can not locate your DATA directory")
        quit()

shell.prefix("set -eo pipefail; ")
configfile: NGS_PIPELINE +"/config_common.json"
configfile: NGS_PIPELINE +"/config_cluster.json"
###########################################################################
config['germlineCallers'] = [ "hapcaller", "bam2mpg", "platypus"]
config['somaticCallers'] = ["MuTect", "strelka"]
###########################################################################
SUBJECT_TO_SAMPLE  = {}
for subject in config['subject']:
        SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['subject'][subject])
###########################################################################
SAMPLE_TO_SUBJECT  = {}
for subject,samples in config['subject'].items():
    for sample in samples:
        SAMPLE_TO_SUBJECT[sample]=subject
###########################################################################
####
#### Targets
####
SUBS  = []
for subject,samples in config['subject'].items():
	SUBS.append(subject)

ALL_SAMPLES = config["samples"]
ALL_FASTQC  = ["{subject}/{sample}/qc/fastqc/{sample}_R2_fastqc.html".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['samples']]
ALL_BAMS    = ["{subject}/{sample}/{sample}.bwa.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['samples']]
ALL_QC      = ["{subject}/{sample}/qc/{sample}.bwa.flagstat.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['samples']] 
ALL_QC     += ["{subject}/{sample}/qc/{sample}.bwa.hotspot.depth".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['samples']] 
ALL_QC     += ["{subject}/{sample}/qc/{sample}.bwa.gt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['samples']] 
ALL_QC     += ["{subject}/{sample}/qc/BamQC/qualimapReport.html".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['samples']]
###########################################################################
SUB_BAMS= {} 
SUB_COV = {}
SUB_LOH = {}
SUB_MPG = {}
SUB_HOT = {}
for Subject  in config['subject']:
	SUB_BAMS[Subject]= ["{subject}/{sample}/{sample}.bwa.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][Subject]]
	SUB_COV[Subject] = ["{subject}/{sample}/qc/{sample}.bwa.coverage.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][Subject]]
	SUB_HOT[Subject] = ["{subject}/{sample}/qc/{sample}.bwa.hotspot.depth".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][Subject]]
	SUB_LOH[Subject] = ["{subject}/{sample}/qc/{sample}.bwa.loh".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][Subject]]
	SUB_MPG[Subject] = ["{subject}/{sample}/calls/{sample}.bam2mpg.vcf.gz".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][Subject]]
###########################################################################
somaticPairs = {}
pairedCapture = {}
if len(config['sample_references']) > 0:
	for Tumor in config['sample_references']:
		for Normal in config['sample_references'][Tumor]:
			TumorBam = "{subject}/{sample}/{sample}.bwa.final".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
			NormalBam = "{subject}/{sample}/{sample}.bwa.final".format(subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
			pairedCapture[Tumor] = config['sample_captures'][Tumor]
			somaticPairs[Tumor] = [TumorBam + ".bam" , TumorBam + ".bam.bai", NormalBam + ".bam", NormalBam + ".bam.bai"]
###########################################################################

def add_to_SUBJECT_ANNO(category,file_list):
	for subject in SUBJECT_ANNO:
		if category not in SUBJECT_ANNO[subject]:
			SUBJECT_ANNO[subject][category] = file_list
		else:
			SUBJECT_ANNO[subject][category].extend(file_list)
SUBJECT_ANNO = dict([(key, {}) for key in SUBS])

###########################################################################
SUBJECT_VCFS = {}
SOMATIC =[]
for sample in config['samples']:
	local  = []
	subject=SAMPLE_TO_SUBJECT[sample]
	local.extend([(subject+"/"+subject+"/calls/"+subject+".platypus.snpEff.txt"),
			        (subject+"/"+subject+"/calls/"+subject+".hapcaller.snpEff.txt"),
				(subject+"/"+subject+"/calls/"+subject+".bam2mpg.snpEff.txt")])
	if subject not in SUBJECT_VCFS:
		SUBJECT_VCFS[subject] = local
	germline = [w.replace('snpEff','annotated') for w in local]
	add_to_SUBJECT_ANNO("germline",germline)	

for sample in config['sample_references'].keys():
	local  = []
	subject=SAMPLE_TO_SUBJECT[sample]
	local.extend([ (subject+"/"+sample+"/calls/"+sample+".MuTect.snpEff.txt"),
				 (subject+"/"+sample+"/calls/"+sample+".strelka.snvs.snpEff.txt"),
				 (subject+"/"+sample+"/calls/"+sample+".strelka.indels.snpEff.txt")])
	SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".MuTect.annotated.txt"]
	SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".strelka.snvs.annotated.txt"]
	SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".strelka.indels.annotated.txt"]
        if subject in SUBJECT_VCFS:
                 SUBJECT_VCFS[subject].extend(local)
	somatic = [w.replace('snpEff','annotated') for w in local]
	add_to_SUBJECT_ANNO("somatic",somatic)
###########################################################################
###########################################################################
ALL_EXPRESSED =[]
expressedPairs = {}
if len(config['sample_RNASeq']) > 0:
	for Tumor in config['sample_RNASeq']:
		for RNASample in config['sample_RNASeq'][Tumor]:
			subject=SAMPLE_TO_SUBJECT[Tumor]
			RNASeqBam    = subject + "/" + RNASample + "/"+RNASample+".star.final.bam"
			expressedPairs[Tumor] = RNASeqBam
			ALL_EXPRESSED += ["{subject}/{sample}/calls/{sample}.MuTect.annotated.expressed.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor],  sample=Tumor)]
			ALL_EXPRESSED += ["{subject}/{sample}/calls/{sample}.strelka.snvs.annotated.expressed.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)]

###########################################################################
#				RNASeq Rules				  # 
###########################################################################
include: NGS_PIPELINE +"/rnaseq_pipeline.rules"
include: NGS_PIPELINE +"/scripts/igv_snapshot.rules"
###########################################################################
localrules: Khanlab_Pipeline, RNASeq, IGV_Session, DBinput 
rule Khanlab_Pipeline:
	input: 
		"QC_AllSamples.txt",
		expand("{subject}/qc/{subject}.coveragePlot.png", subject= SUBS),
		expand("{subject}/qc/{subject}.circos.png", subject= SUBS),
		expand("{subject}/qc/{subject}.hotspot_coverage.png", subject= SUBS),
		expand("{subject}/{subject}/calls/{subject}.{caller}.annotated.txt", subject= SUBS, caller=config['germlineCallers']),
		expand("{subject}/{subject}/db/{subject}.{group}", subject= SUBS, group=['germline', 'somatic', 'rnaseq',]),
		SOMATIC,
		ALL_EXPRESSED,
		"RNASeq.done",
		expand("{subject}/igv/session_{subject}.xml",subject= SUBS),
		wait4job= NGS_PIPELINE + "/scripts/" + config["block"],
	output:
		"Status"
	log: "log/FINAL"
	version: "1.0"
	message: "Final Rule, Genotyping"
	shell: """
	#######################
	echo "Pipeline Finished Successfully!!"	 >{output}
	umask 007
	chgrp -R khanlab *
#	cd /data/khanlab/projects/Genotyping/
#	Final=`sh genotype.sh`
#	perl {input.wait4job} ${{Final}}
	#######################
	"""
############
#	FASTQC
############
rule fastqc:
	input:
		R1=DATA_DIR + "/{sample}/{sample}_R1.fastq.gz",
		R2=DATA_DIR + "/{sample}/{sample}_R2.fastq.gz"
	output: 
		"{base}/{sample}/qc/fastqc/{sample}_R1_fastqc.html", 
		"{base}/{sample}/qc/fastqc/{sample}_R2_fastqc.html"
	log: "log/fastqc.{sample}"
	version: config["fastqc"]
	message: "Running Fastqc on {wildcards.sample}"
	params:
		rulename  = "fastqc",
		batch     = config["job_fastqc"]
	shell: """
	#######################
	module load fastqc/{version}
	fastqc --extract -t 6 -o {wildcards.base}/{wildcards.sample}/qc/fastqc/ -d /scratch {input[R1]} 2>> {log}
	fastqc --extract -t 6 -o {wildcards.base}/{wildcards.sample}/qc/fastqc/ -d /scratch {input[R2]} 2>> {log}
	#######################
	"""
############
#       BWA
############
rule BWA:
	input: 
		R1=DATA_DIR + "/{sample}/{sample}_R1.fastq.gz", 
		R2=DATA_DIR + "/{sample}/{sample}_R2.fastq.gz",
		ref=config["bwaIndex"]
	output: 
		temp("{base}/{sample}/{sample}.bwa.bam"),
		temp("{base}/{sample}/{sample}.bwa.bam.bai")
	log: "log/bwa.{sample}"
	version: config["bwa"]
	message: "Running bwa on {wildcards.sample}"
	params:
		rulename  = "bwa",
		platform  = config["platform"],
		samtools  = config["samtools"], 
		batch     = config["job_bwa"]
	shell: """
	#######################
	module load bwa/{version}
	module load samtools/{params.samtools}
	bwa mem -M \
	-t ${{SLURM_CPUS_ON_NODE}}\
	-R '@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tLB:{wildcards.sample}\tPL:{params.platform}' \
	{input.ref} {input.R1} {input.R2} 2> {log} | samtools view -Sbh - \
	| samtools sort -m 30000000000 - {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa
	samtools index {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa.bam
	#######################
	"""
############
#	Novoalign
############
rule Novoalign:
	input:
		R1=DATA_DIR + "/{sample}/{sample}_R1.fastq.gz",
		R2=DATA_DIR + "/{sample}/{sample}_R2.fastq.gz",
		index=config["Novo_index"]
	output:
		temp("{subject}/{sample}/{sample}.novo.bam"),
		temp("{subject}/{sample}/{sample}.novo.bam.bai")
	log: "log/novoalign.{sample}"
	version: config["novocraft"]
	message: "Running Novoalign on {wildcards.sample}"
	params:
		rulename  = "novoalign",
		batch     = config["job_novoalign"],
		samtools  = config["samtools"],
		platform  = config["platform"]
	shell: """
	#######################
	module load samtools/{params.samtools}
	module load novocraft/{version}
	mpiexec  -envall -host `scontrol show hostname ${{SLURM_NODELIST}} | paste -d',' -s` -np ${{SLURM_NTASKS}} novoalignMPI -F STDFQ -o SAM \"@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}\" -t 250 --hlimit 7 -p 5,2 -l 30 -e 100 -i 200 100 -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -H -d {input.index} -f {input.R1} {input.R2} 2>{log}| samtools view -uS - | samtools sort -m 30000000000 - {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo 
	samtools index {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo.bam 
	#######################
	"""
############
#       GenotypeFile
############
# Using Older version of samtools for this purpose
rule Genotyping:
	input:
		bam="{base}/{sample}/{sample}.{aligner}.final.bam",
		interval=config["genotypeBed"],
		ref=config["reference"],
		vcf2genotype=NGS_PIPELINE + "/scripts/" + config["vcf2genotype"],
		vcf2loh=NGS_PIPELINE + "/scripts/" + config["vcf2loh"],
	output:
		vcf="{base}/{sample}/calls/{sample}.{aligner}.samtools.vcf",
		gt="{base}/{sample}/qc/{sample}.{aligner}.gt",
		loh="{base}/{sample}/qc/{sample}.{aligner}.loh"
	log: "log/genotyping.{sample}"
	version: config["samtools_old"]
	message: "Making Genotyping Input file on {input.bam}"
	params:
		rulename  = "genotype",
		batch     = config["job_genotype"],
		dest	  = config["genotypeDest"]
	shell: """
	#######################
	module load samtools/{version}
	samtools mpileup -u -C50 -f {input.ref} -l {input.interval} {input.bam} | bcftools view -gc - >{output.vcf} 2>{log}
	perl {input.vcf2genotype} {output.vcf} >{output.gt} 2>>{log}
	cp -f {output.gt} {params.dest}/{wildcards.sample}.gt 2>>{log}
	perl {input.vcf2loh} {output.vcf} >{output.loh} 2>>{log}
	#######################
	"""	
############
#       BamQC
############
rule BamQC:
	input:
		bam="{base}/{sample}/{sample}.bwa.final.bam",
		bai="{base}/{sample}/{sample}.bwa.final.bam.bai",
		interval= lambda wildcards: config['sample_captures'][wildcards.sample].replace('.bed', '.gff')
	output:
		"{base}/{sample}/qc/BamQC/qualimapReport.html"
	log: "log/bamqc.{sample}"
	version: config["qualimap"]
	message: "Running BamQC on {wildcards.sample}"
	params: 
		rulename = "bamqc",
		batch	 = config["job_qualimap"],
		outdir	 ="{base}/{sample}/qc/BamQC",
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load qualimap/{version}
	qualimap bamqc -c -bam {input.bam} -outdir {params.outdir} -gff {input.interval} -nt ${{SLURM_CPUS_ON_NODE}} --java-mem-size=${{MEM}}G > {log} 2>&1
	#######################
	"""
############
#       QC_Sum
############
rule QC_Sum:
	input:
		ALL_QC,
		ALL_FASTQC,
		convertor = NGS_PIPELINE + "/scripts/" + config["makeQC"]
	output:
		"QC_AllSamples.txt"
	log: "log/qc_sum"
	version: "v1.1"
	message: "Collecting QC on all samples"
	params:
		rulename = "qc_sum",
		batch    = config['job_default']
	shell: """
	#######################
	perl {input.convertor} `pwd` >{output} 2>{log}
	#######################
	"""
############
#       Samtools flagstat
############
rule flagstat:
	input:	"{base}/{sample}/{sample}.{aligner}.final.bam"
	output: "{base}/{sample}/qc/{sample}.{aligner}.flagstat.txt"
	log:    "log/flagstat.{sample}"
	version: config["samtools"]
	message: "Running samtools flagstat on {wildcards.sample}"
	params:
		rulename  = "flagstat",
		batch     = config["job_flagstat"]
	shell: """
	#######################
	module load samtools/{version}
	samtools flagstat {input} > {output}
	#######################
	"""
############
#       Hotspot Coverage
############
rule HotSpotCoverage:
	input:	
		bam="{base}/{sample}/{sample}.{aligner}.final.bam",
		interval=config["hotspot_intervals"]
	output: "{base}/{sample}/qc/{sample}.{aligner}.hotspot.depth"
	log: "log/hotspotCoverage.{sample}"
	version: config["bedtools"]
	message: "Running Hotspot Coverage on {wildcards.sample}"
	params:
		rulename  = "HotSpotCov",
		batch     = config["job_hotspot"],
		samtools  = config["samtools"]
	shell: """
	#######################
	module load samtools/{params.samtools} 
	module load bedtools/{version}
	samtools view -hF 0x400 -q 30 {input.bam} | samtools view -ShF 0x4 - | samtools view -SuF 0x200 - | bedtools coverage -abam - -b {input.interval} >{output}
	#######################
	"""
############
# Coverage 
############
rule Coverage:
	input:
		bam="{subject}/{sample}/{sample}.bwa.final.bam",
		bai="{subject}/{sample}/{sample}.bwa.final.bam.bai",
		interval= lambda wildcards: config['sample_captures'][wildcards.sample]
	output:
		"{subject}/{sample}/qc/{sample}.bwa.coverage.txt"
	log: "log/coverage.{sample}"
	version: config["bedtools"]
	message: "Running Coverage on {input.bam}"
	params:
		rulename = "coverage",
		batch    = config["job_bedtools"]
	shell: """
	#######################
	module load bedtools/{version}
	bedtools coverage -abam {input.bam} -b {input.interval} -hist |grep "^all" > {output}
	#######################
	"""
############
# CoveragePlot
############
rule CoveragePlot:
	input: 
		covFiles=lambda wildcards: SUB_COV[wildcards.subject],
		coverage =NGS_PIPELINE + "/scripts/" + config["coverage"]
	output: "{subject}/qc/{subject}.coveragePlot.png",
	log: "log/coverage.{subject}"
	version: config["R"]
	message: "Generating Coverage Plot on {wildcards.subject}"
	params:
		rulename = "covplot",
		batch    = config["job_covplot"]
	shell: """
	#######################
		
	cp -f {input.covFiles} {wildcards.subject}/qc/ 
		
	module load R/{version}
	R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.coverage}
	#######################
	"""
############
# Circos Plot
############
rule Circos:
	input:
		lohFiles=lambda wildcards: SUB_LOH[wildcards.subject],
		circos =NGS_PIPELINE + "/scripts/" + config["circos"]
	output:
		"{subject}/qc/{subject}.circos.png",
	log: "log/circos.{subject}"
	version: config["R"]
	message: "Generating CIRCOS Plot on {wildcards.subject}"
	params:
		rulename = "Circos",
		batch    = config["job_covplot"]
	shell: """
	#######################
	cp -f {input.lohFiles} {wildcards.subject}/qc/
	module load R/{version}
	R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.circos}
	#######################
	"""
############
# Box Plot Hotspot
############
rule BoxPlot_Hotspot:
	input:
		covFiles=lambda wildcards: SUB_HOT[wildcards.subject],
		boxplot =NGS_PIPELINE + "/scripts/" + config["boxplot"]
	output:
		"{subject}/qc/{subject}.hotspot_coverage.png",
	log: "log/boxplot.{subject}"
	version: config["R"]
	message: "Generating BoxPlot Plot on {wildcards.subject}"
	params:
		rulename = "Boxplot",
		batch    = config["job_covplot"]
	shell: """
	#######################
	cp -f {input.covFiles} {wildcards.subject}/qc/
	module load R/{version}
	R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.boxplot}
	#######################
	"""
############
#       Picard Mark Duplicates
############
rule Picard_MarkDup:
	input:
		bam="{subject}/{sample}/{sample}.{base}.bam",
		bai="{subject}/{sample}/{sample}.{base}.bam.bai"
		
	output: 
		bam=temp("{subject}/{sample}/{sample}.{base}.dd.bam"),
		index=temp("{subject}/{sample}/{sample}.{base}.dd.bam.bai"),
		metrics="{subject}/{sample}/qc/{base}.markdup.txt"
	log:    "log/markdup.{sample}.{base}"
	version: config["picard"]
	message: "Running picard mark duplicates on {wildcards.sample}"
	params:
		rulename  = "mark_dup",
		batch     = config["job_markdup"],
		samtools  = config["samtools"]    
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load picard/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $PICARDJARPATH/MarkDuplicates.jar AS=true M={output.metrics} I={input.bam} O={output.bam} REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT > {log} 2>&1
	module load samtools/{params.samtools}
	samtools index {output.bam}
	######################
	"""
############
#       GATK Best Practices
############
rule GATK:
	input: 	bam="{base}/{sample}/{sample}.bwa.dd.bam",
		bai="{base}/{sample}/{sample}.bwa.dd.bam.bai",
		ref=config["reference"],
		phase1=config["1000G_phase1"],
		mills=config["Mills_and_1000G"]
	output:
		bam="{base}/{sample}/{sample}.bwa.final.bam",
		index="{base}/{sample}/{sample}.bwa.final.bam.bai",
	log:    "log/gatk.{sample}"
	version: config["GATK"]
	message: "Running GATK Best practices on {input.bam}"
	params:
		rulename  = "gatk",
		batch     = config["job_gatk"]
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load GATK/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T RealignerTargetCreator -R {input.ref} -known {input.phase1} -known {input.mills} -I {input.bam} -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.realignment.intervals > {log} 2>&1
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T IndelRealigner -R {input.ref} -known {input.phase1} -known {input.mills} -I {input.bam} --targetIntervals /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.realignment.intervals -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam >>{log} 2>&1
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T BaseRecalibrator -R {input.ref} -knownSites {input.phase1} -knownSites {input.mills} -I /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.recalibration.matrix.txt >>{log} 2>&1
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T PrintReads -R {input.ref} -I /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam -o {output.bam} -BQSR /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.recalibration.matrix.txt >>{log} 2>&1
	mv -f {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa.final.bai {output.index}
	######################
	"""
############
#	Bam2MPG
############
rule Bam2MPG:
	input:
		bam="{subject}/{sample}/{sample}.novo.dd.bam",
		bai="{subject}/{sample}/{sample}.novo.dd.bam.bai",
		ref=config["reference"],
		interval= lambda wildcards: config['sample_captures'][wildcards.sample],
	output:
		snps="{subject}/{sample}/calls/{sample}.bam2mpg.vcf.gz",
	log: "log/bam2mpg.{sample}"
	version: config["bam2mpg"]
	message: "Running Bam2MPG on {input.bam}"
	params:
		rulename  = "bam2mpg",
		batch     = config["job_bam2mpg"],
		samtools  = config["samtools"],
		vcftools  = config["vcftools"]
	shell: """
	#######################
	if [ -f {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.gz ]; then
        	rm -rf {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.*
		
	fi

	module load bam2mpg/{version}
	module load vcftools/{params.vcftools}	
	for CHR in `seq 1 22` X Y;
	do
	bam2mpg --qual_filter 20 -bam_filter '-q31' --region chr${{CHR}} --only_nonref --snv_vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf --div_vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf {input.ref} {input.bam} &
	done
	wait

	for CHR in `seq 1 22` X Y
	do
		cat /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf | vcf-sort >/lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf
		mv -f /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf
		bgzip /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf
		cat /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf | vcf-sort >/lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf
		mv -f /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf
		bgzip /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf
		tabix -p vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf.gz
		tabix -p vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf.gz
	done
	echo "Combine chr level vcf files"
	vcf-concat /lscratch/${{SLURM_JOBID}}/chr*{wildcards.sample}.*.vcf.gz >/lscratch/${{SLURM_JOBID}}/{wildcards.sample}.snps.vcf
	echo "Restrict to Bed file"

	vcftools --vcf /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.snps.vcf --bed {input.interval} --out {wildcards.sample} --recode --keep-INFO-all

	sed -e 's/SAMPLE/{wildcards.sample}/g' {wildcards.sample}.recode.vcf |vcf-sort >{wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf

	bgzip {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf
	tabix -f -p vcf {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.gz
	rm -rf {wildcards.sample}.recode.vcf
	
	#######################
	"""
############
# Subject Bam2MPG
############
rule Sub_MPG:
	input:
		vcf=lambda wildcards: SUB_MPG[wildcards.subject],
#		convertor= NGS_PIPELINE + "/scripts/" + config["vcffilter"]
	output:
		vcf="{subject}/{subject}/calls/{subject}.bam2mpg.raw.vcf"
	log: "log/mpg.{subject}"
	version: config["vcftools"]
	message: "Merge sample level mpg files to subject level {subject}"
	params:
		rulename = "mergevcf",
		batch    = config["job_default"],
		vcforder = NGS_PIPELINE + "/scripts/" +config["vcfOrderCol"]
	shell: """
	#######################
	module load vcftools/{version}
	module load R
	vcf-merge {input.vcf} > {output.vcf}.tmp
	{params.vcforder} -i {output.vcf}.tmp -o {output.vcf}
	rm -rf {output.vcf}.tmp
	#######################
	"""
############
#       MuTect
############
rule MuTect:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		cosmic=config["cosmic"],
		interval=pairedCapture[Tumor]
	output:
		vcf="{subject}/{Tumor}/calls/{Tumor}.MuTect.raw.vcf",
		call_stats="{subject}/{Tumor}/qc/{Tumor}.mutect.call_stats.txt",
		coverage="{subject}/{Tumor}/qc/{Tumor}.mutect.coverage.wig.txt"
	log:	"log/mutect.{Tumor}"
	version: config["MuTect"]
	message: "MuTect on {wildcards.Tumor}"
	params:
		rulename = "MuTect",
		batch    = config["job_mutect"],
		vcforder = NGS_PIPELINE + "/scripts/" +config["vcfOrderCol"]
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	gawk '{{print $1 "\t" $2-1 "\t" $3}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
	module load muTect/{version}
	module load R
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $MUTECTJARPATH -T MuTect \
		--reference_sequence {input.ref} \
		--cosmic {input.cosmic} \
		--dbsnp {input.dbsnp} \
		--input_file:normal {input[2]} \
		--input_file:tumor {input[0]} \
		--intervals  /lscratch/${{SLURM_JOBID}}/target_intervals.bed \
		--coverage_file {output.coverage} \
		--out {output.call_stats} \
		--vcf {output.vcf}.vcf \
		> {log} 2>&1
	
	{params.vcforder} -i {output.vcf}.vcf -o {output.vcf}
	rm -rf {output.vcf}.vcf
	#######################
	"""
############
#       Strelka
############
rule Strelka:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		config=config["strelka_config"],
		interval=pairedCapture[Tumor]
	output:
		snps="{subject}/{Tumor}/calls/{Tumor}.strelka.snvs.raw.vcf",
		indels="{subject}/{Tumor}/calls/{Tumor}.strelka.indels.raw.vcf"
	log:    "log/strelka.{Tumor}"
	version: config["strelka"]
	message: "Strelka on {wildcards.Tumor}"
	params:
		rulename = "Strelka",
		batch    = config["job_strelka"],
		vcftools = config["vcftools"]
	shell: """
	#######################
	module load strelka/{version}
	configureStrelkaWorkflow.pl --normal={input[2]} --tumor={input[0]}\
	--ref={input.ref} --config={input.config} --output-dir=/lscratch/${{SLURM_JOBID}}/strelka > {log} 2>&1
	make -j ${{SLURM_CPUS_PER_TASK}} -f /lscratch/${{SLURM_JOBID}}/strelka/Makefile 2>> {log}
	module load vcftools/{params.vcftools}
	vcftools --vcf /lscratch/${{SLURM_JOBID}}/strelka/results/passed.somatic.snvs.vcf --bed {input.interval} --out {output.snps} --recode --keep-INFO-all
	mv -f {output.snps}.recode.vcf {output.snps}
	vcftools --vcf /lscratch/${{SLURM_JOBID}}/strelka/results/passed.somatic.indels.vcf --bed {input.interval} --out {output.indels} --recode --keep-INFO-all
	mv -f {output.indels}.recode.vcf {output.indels}
	NORMAL=`basename {input[2]} .bwa.final.bam`
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.snps}
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.indels}
	
	#######################
	"""
############
# Subject Hapcaller
############
rule Sub_HapCall:
	input:
		bams=lambda wildcards: SUB_BAMS[wildcards.subject],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		interval= lambda wildcards: config['sample_captures'][wildcards.subject]
	output:
		vcf="{subject}/{subject}/calls/{subject}.hapcaller.raw.vcf"
	log: "log/hapcaller.{subject}"
	version: config["GATK"]
	message: "Haplotype Caller on {wildcards.subject}"
	params:
		rulename = "HC",
		batch    = config["job_gatk"]
	run:
		bamFiles = " ".join('-I ' + bam for bam in input.bams)
		shell("""
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load GATK/{version}
	gawk '{{print $1 "\t" $2-1 "\t" $3}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T HaplotypeCaller -R {input.ref} {bamFiles} -L /lscratch/${{SLURM_JOBID}}/target_intervals.bed -o {output.vcf} --dbsnp {input.dbsnp} -mbq 20 -mmq 30 -log {log}
	#######################
	""")
############
# Subject Platypus
############
rule Sub_Platypus:
	input:
		bams=lambda wildcards: SUB_BAMS[wildcards.subject],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		interval= lambda wildcards: config['sample_captures'][wildcards.subject]
	output:
		vcf="{subject}/{subject}/calls/{subject}.platypus.raw.vcf"
	log: "log/platypus.{subject}"
	version: config["platypus"]
	message: "Platypus on {wildcards.subject}"
	params:
		rulename = "PLAT",
		batch    = config["job_platypus"]
	shell: """
	#######################
	module load platypus/{version}
	LIST=`echo {input.bams}|sed -e 's/ /,/g'`
	platypus callVariants --nCPU=${{SLURM_CPUS_ON_NODE}} --bufferSize=1000000 --maxReads=100000000 --bamFiles=${{LIST}} --regions={input.interval} --output={output.vcf} --refFile={input.ref} --logFileName={log}
	sed -i 's/.bwa.final//g' {output.vcf}
	#######################
	"""
############
#       FreeBayes 	** Not in use **
############
rule  FreeBayes:
	input:
		bam="{subject}/{sample}/{sample}.bwa.final.bam",
		bai="{subject}/{sample}/{sample}.bwa.final.bam.bai",
		ref=config["reference"],
		interval= lambda wildcards: config['sample_captures'][wildcards.sample]
	output:
		vcf="{subject}/{sample}/calls/{sample}.freebayes.vcf"
	log:    "log/freebayes.{sample}"
	version: config["freebayes"]
	message: "Freebayes on {wildcards.subject}"
	params:
		rulename = "freebayes",
		batch    = config["job_freebayes"],
		vcftools = config["vcftools"]
	shell: """
	#######################
	module load freebayes/{version}
	freebayes -f {input.ref} --haplotype-length 50 -b {input.bam} -v {output.vcf} > {log} 2>&1
	module load vcftools/{params.vcftools}
	vcftools --vcf {output} --bed {input.interval} --out {output.vcf} --recode --keep-INFO-all
	mv -f {output.vcf}.recode.vcf {output.vcf}
	#######################
	"""
############
#	snpEff
############
rule snpEff:
	input:
		vcf="{subject}/calls/{base}.raw.vcf",
		ref=config["reference"],
		snpEff_config=config["snpEff_config"],
	output:
		eff="{subject}/calls/{base}.raw.snpEff.vcf"
	log: "log/snpEff.{base}"
	version: config["snpEff"]
	message: "snpEff on {input.vcf}"
	params:
		rulename      ="snpEff",
		batch	      =config["job_snpeff"],
		snpEff_genome =config["snpEff_genome"],
		annovar       =config["annovar"]
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load snpEff/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar ${{SNPEFFHOME}}/SnpSift.jar dbnsfp -c {input.snpEff_config} -a {input.vcf} | java -Xmx${{MEM}}g -jar ${{SNPEFFHOME}}/snpEff.jar -t -canon {params.snpEff_genome} > {output.eff} 2> {log}
	#######################
	"""
############
#       vcf2txt
############
rule vcf2txt:
	input:
		eff="{subject}/calls/{base}.raw.snpEff.vcf",
		vcf2txt=NGS_PIPELINE + "/scripts/" + config["vcf2txt"]
	output:
		txt="{subject}/calls/{base}.snpEff.txt"
	params:
		rulename      ="snpEff",
		batch         =config["job_default"],
		annovar       =config["annovar"]
	shell: """
	#######################
	module load annovar/{params.annovar}
	perl {input.vcf2txt} {input.eff} /spin1/scratch/ >{output.txt}
	#######################
	"""

############
#	MakeList
############
rule FormatInput:
	input:
		txtFiles=lambda wildcards: SUBJECT_VCFS[wildcards.subject],
		convertor= NGS_PIPELINE + "/scripts/" + config["annoInput"]
	output: 
		"{subject}/annotation/AnnotationInput.anno", 
		"{subject}/annotation/AnnotationInput.pph",
		"{subject}/annotation/AnnotationInput.sift",
	log: "log/FormatInput.{subject}"
	version: config["annovar"]
	message: "Making Annotation input {wildcards.subject}"
	params:
		rulename   = "FormatInput",
		batch      = config["job_default"],
		fAEV       = NGS_PIPELINE + "/scripts/" + config["fAEV"]
	shell: """
	#######################
	module load annovar/{version}
	cut -f 1-5 {input.txtFiles} |sort |uniq > {wildcards.subject}/annotation/allSites
	perl {params.fAEV} {wildcards.subject}/annotation/allSites annovar/AnnotationInput.final.txt {wildcards.subject}/annotation/AnnotationInput.annotations.final.txt {wildcards.subject}/annotation/AnnotationInput
	perl {input.convertor} {wildcards.subject}/annotation/AnnotationInput 2>{log}
	#######################
	"""
############
#	Custom Annotation
############
rule Annotation:
	input: 
		"{subject}/annotation/AnnotationInput.anno",
		TableAnnovar=NGS_PIPELINE + "/scripts/" + config["Annovar"],
		custom     =NGS_PIPELINE + "/scripts/" + config["customAnno"]
	output:
		"{subject}/annotation/AnnotationInput.docm"
	log: "log/Annovar.{subject}"
	version: config["annovar"]
	message: "Custom annotation on {wildcards.subject}"
	params:
		rulename   = "Annotation",
		batch      = config["job_annovar"],
		RefData    = config["annovar_data"],
		build      = config["build"],
	shell: """
	#######################
	module load annovar/{version}
	sh {input.TableAnnovar} {wildcards.subject}/annotation AnnotationInput {input.custom} 2>>{log}
	#######################
	"""
############
#       SIFT
############
rule SIFT:
	input:
		sift="{subject}/annotation/AnnotationInput.sift",
		convertor  = NGS_PIPELINE + "/scripts/" + config["SiftParse"]
	output: "{subject}/annotation/AnnotationInput.sift.out"
	log: "log/SIFT.{subject}"
	version: config["SIFT"]
	message: "SIFT on {wildcards.subject}"
	params:
		rulename   = "SIFT",
		batch      = config["job_SIFT"],
		build      = config["SIFTbuild"]
	shell: """
	#######################
	module load python/2.7.4
	module load SIFT/{version}
	DIR=`pwd`
	cd ${{DIR}}/`dirname {input.sift}`
	FILE=`basename {input.sift}`
	SIFT_exome_nssnvs.pl -i ${{FILE}} -d $SIFTDB/Human_db_37 -o $SIFT_SCRATCHDIR -z ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv
	perl {input.convertor} ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv >${{DIR}}/`dirname {input.sift}`/${{FILE}}.out
	#######################
	"""
############
#       PPH2
############
rule PPH2:
	input:
		pph="{subject}/annotation/AnnotationInput.pph",
		convertor  = NGS_PIPELINE + "/scripts/" + config["PPH2Parse"],
		block      = NGS_PIPELINE + "/scripts/" + config["block"]
	output: "{subject}/annotation/AnnotationInput.pph2.out"
	log: "log/PPH2.{subject}"
	version: config["polyphen2"]
	message: "pph2 on {wildcards.subject}"
	params:
		rulename   = "PPH2",
		batch      = config["job_PPH2"],
	shell: """
	#######################
	module load polyphen2/{version}
	if [ -s {input.pph} ]; then
		mapsnps.pl -c -g hg19 -U -y {input.pph}.intermediate {input.pph} 2>{log}
		pph_swarm.pl {input.pph}.intermediate -d /scratch/`whoami`/${{RANDOM}}${{RANDOM}} -o {wildcards.subject}/annotation/AnnotationInput.pph2.intermediate.txt --partition ${{SLURM_JOB_PARTITION}} --block
		perl {input.convertor}  {wildcards.subject}/annotation/AnnotationInput.pph2.intermediate.txt >{output} 2>>{log}
	else 
		touch {output}
	fi
	rm -f {wildcards.subject}/annotation/AnnotationInput.pph.inter*	
	#######################
	"""
############
#	Combine Annotation
############
rule CombineAnnotation:
	input:
		anno="{subject}/annotation/AnnotationInput.docm", 
		sift="{subject}/annotation/AnnotationInput.sift.out", 
		pph2="{subject}/annotation/AnnotationInput.pph2.out",
		convertor  = NGS_PIPELINE + "/scripts/" + config["CombineAnno"],
		geneanno   = NGS_PIPELINE + "/scripts/" + config["GeneAnno"],
		filter     = NGS_PIPELINE + "/scripts/" + config["filterVars"]
	output: "{subject}/annotation/AnnotationInput.annotations.final.txt.tmp"
	log: "log/CombineAnnotation.{subject}"
	version: "1.0"
	message: "Merge Annotation {wildcards.subject}"
	params:
		rulename   = "combine",
		batch	   = config["job_Combine"],
		dataDir    = config["annovar_data"]
	shell: """
	#######################
	echo "{wildcards.subject}/annotation/AnnotationInput
{wildcards.subject}/annotation/AnnotationInput.anno.gene
{wildcards.subject}/annotation/AnnotationInput.anno.exac.3
{wildcards.subject}/annotation/AnnotationInput.anno.clinseq
{wildcards.subject}/annotation/AnnotationInput.anno.cadd
{wildcards.subject}/annotation/AnnotationInput.sift.out
{wildcards.subject}/annotation/AnnotationInput.pph2.out
{wildcards.subject}/annotation/AnnotationInput.anno.clinvar
{wildcards.subject}/annotation/AnnotationInput.hgmd
{wildcards.subject}/annotation/AnnotationInput.match
{wildcards.subject}/annotation/AnnotationInput.docm
{wildcards.subject}/annotation/AnnotationInput.candl
{wildcards.subject}/annotation/AnnotationInput.tcc
{wildcards.subject}/annotation/AnnotationInput.mcg
{wildcards.subject}/annotation/AnnotationInput.civic
{wildcards.subject}/annotation/AnnotationInput.anno.pcg" >{wildcards.subject}/annotation/list
	perl {input.convertor} {wildcards.subject}/annotation/list >{output}
	perl {input.geneanno} {params.dataDir}/hg19_ACMG.txt {output} >>{subject}/annotation/AnnotationInput.annotations.final.txt
	perl {input.filter} {subject}/annotation/AnnotationInput.annotations.final.txt 0.1 >{subject}/annotation/AnnotationInput.annotations.final.txt.hold
	mv -f {subject}/annotation/AnnotationInput.annotations.final.txt.hold {subject}/annotation/AnnotationInput.annotations.final.txt
	#######################
	"""
############
#	Add Annotation back to sample level file 
############
rule AttachAnnotation:
	input:
		txt="{subject}/{base1}/calls/{base}.snpEff.txt",
		ref="{subject}/annotation/AnnotationInput.annotations.final.txt.tmp",
		convertor  = NGS_PIPELINE + "/scripts/" + config["addBack"]
	output:
		txt="{subject}/{base1}/calls/{base}.annotated.txt"
	log: "log/attach_annotation.{subject}.{base}"
	version: "1.0"
	message: "Annotate {input.txt}"
	params:
		rulename   = "add",
		batch      = config["job_addbackann"],
	shell: """
	#######################
	perl {input.convertor} {subject}/annotation/AnnotationInput.annotations.final.txt  {input.txt} >{output.txt} 2>{log}
	#######################
	"""
############
#       Expressed
############
rule Expressed:
	input: 
		RNASeq = lambda wildcards: expressedPairs[wildcards.sample],
		Mutation="{subject}/{sample}/calls/{base}.annotated.txt",
		convertor = NGS_PIPELINE + "/scripts/" + config["mpileup"]
	output: "{subject}/{sample}/calls/{base}.annotated.expressed.txt"
	log: "log/expressed.{base}"
	version: config["samtools"]
	message: "Running Expressed mpileup on {wildcards.sample} {wildcards.base}"
	params:
		rulename  = "Expressed",
		batch     = config["job_expressed"],
		convertor = config["mpileup"]
	shell: """
	#######################
	module load samtools/{version}
	perl {input.convertor} {input.Mutation} {input.RNASeq} > {output} 2>{log}
	#######################
	"""
############
#       Database Input
############
rule DBinput:
	input:
		txtFiles=lambda wildcards: SUBJECT_ANNO[wildcards.subject][wildcards.group],
		convertor=NGS_PIPELINE + "/scripts/" + config["DBinput"]
	output: "{subject}/{subject}/db/{subject}.{group}"
	params:
		rulename  = "makeDBinput"
	shell: """
	#######################
	perl {input.convertor} {input.txtFiles} >{output}
	#######################
	"""	
############
#	**END**
############
